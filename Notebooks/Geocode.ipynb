{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('column_space', 40)\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "iso = pd.read_csv('../data/iso_codes.csv')\n",
    "df = pd.read_excel('../data/AIMS MASTER DATA.xlsx', header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in title, year, and author for rows\n",
    "# not containing information via DOI\n",
    "fill_in_cols = ['DOI', 'Q 1.', 'Q 2.', 'Q 3.']\n",
    "tmp = df.loc[:, fill_in_cols]\n",
    "tmp = tmp.drop_duplicates('DOI')\n",
    "df = df.drop(fill_in_cols[1:], axis=1)\n",
    "df = pd.merge(df, tmp, on='DOI', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create row id for later merging\n",
    "df['row_id'] = np.arange(len(df))\n",
    "\n",
    "# Clean deliminators\n",
    "for c in ['Q 11._fix', 'Q 12.', 'Q 13._fix']:\n",
    "    df[c] = df[c].str.replace(' and', ',')\n",
    "    df[c] = df[c].str.replace('-', ',')\n",
    "    df[c] = df[c].str.replace('/', ',')\n",
    "    df[c] = df[c].str.replace('(', ',')\n",
    "    df[c] = df[c].str.replace(')', '')\n",
    "    df[c] = df[c].str.replace(', ,', ',')\n",
    "    df[c] = df[c].str.replace(',,', ',')\n",
    "    df[c] = df[c].str.replace(',', ';')\n",
    "    df[c] = df[c].str.strip()\n",
    "\n",
    "df['Q 11._fix'] = df['Q 11._fix'].str.split(';', expand=False)\n",
    "df['Q 13._fix'] = df['Q 13._fix'].str.split(';', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reshape dataframe so 1 country name to 1 sub-national region\n",
    "# Retained DOI and row_id to later match record\n",
    "\n",
    "for i in xrange(len(df)):\n",
    "    tmp = pd.DataFrame(zip(df['Q 11._fix'][i], df['Q 13._fix'][i]))\n",
    "    tmp.columns = ['ISO', 'Loc']\n",
    "    tmp['DOI'] = df['DOI'][i]\n",
    "    tmp['row_id'] = df['row_id'][i]\n",
    "    if i is 0:\n",
    "        out = tmp.copy()\n",
    "    else:\n",
    "        out = pd.concat([out, tmp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out['Loc'] = out['Loc'].str.strip()\n",
    "out['Loc_orig'] = out['Loc'].copy()\n",
    "\n",
    "# Add country name via ISO match\n",
    "out['ISO'] = out['ISO'].str.strip()\n",
    "iso['ISO'] = iso['ISO'].str.strip()\n",
    "out = pd.merge(out, iso, on='ISO', how='left')\n",
    "\n",
    "# Gets capitol city where country general\n",
    "out['Loc'] = np.where(out['Loc'].str.contains('general'), out['Capitol'], out['Loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean data for particular locations\n",
    "\n",
    "out['Loc'] = np.where((out['Loc'] == 'Rhone River catchment') & (out['ISO'] == 'CHE'), 'Geneva', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Rhone Basin') & (out['ISO'] == 'CHE'), 'Geneva', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Rhine River') & (out['ISO'] == 'AUS'), 'Meiningen', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Caucasus') & (out['ISO'] == 'GEO'), 'Edisa', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Caucasus') & (out['ISO'] == 'AZE'), 'Zagatala State Reserve', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Caucasus') & (out['ISO'] == 'ARM'), 'Alaverdi', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Syr Darya basin') & (out['ISO'] == 'KGZ'), 'Naryn', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Syr Darya basin') & (out['ISO'] == 'UZB'), 'Darvoza', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Kailash Sacred Landscape') & (out['ISO'] == 'NEP'), 'Limi', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Kailash Sacred Landscape') & (out['ISO'] == 'IND'), 'Munsyari', out['Loc'])\n",
    "out['Loc'] = np.where((out['Loc'] == 'Kailash Sacred Landscape') & (out['ISO'] == 'Tibet'), 'Kangrinboqe', out['Loc'])\n",
    "\n",
    "out['Loc'] = out['Loc'].str.replace('Andean Glaciers', 'Glacier')\n",
    "out['Loc'] = out['Loc'].str.replace('Andean glaciers', 'Glacier')\n",
    "out['Loc'] = out['Loc'].str.replace('Columbia River Basin', 'Revelstoke')\n",
    "out['Loc'] = out['Loc'].str.replace('Swiss Alps glacial region', 'Aletsch Glacier')\n",
    "out['Loc'] = out['Loc'].str.replace('Gletschbode Rhone basin', 'Geneva')\n",
    "out['Loc'] = out['Loc'].str.replace('Po Basin', 'Po')\n",
    "out['Loc'] = out['Loc'].str.replace('Kyrgyz catchments', 'Kyrgyzstan general')\n",
    "out['Loc'] = out['Loc'].str.replace('Northern China', 'China general')\n",
    "out['Loc'] = out['Loc'].str.replace('Columbian Andes', 'Bogata')\n",
    "out['Loc'] = out['Loc'].str.replace('Upper Chenab basin', 'Amritsar')\n",
    "out['Loc'] = out['Loc'].str.replace('Khojabakirgansai', 'Khujand')\n",
    "out['Loc'] = out['Loc'].str.replace('Hindu Kush and Himalaya ranges', 'Nepal General')\n",
    "out['Loc'] = out['Loc'].str.replace('Bashkara Glacier Lakes', 'Mount Elbrus')\n",
    "out['Loc'] = out['Loc'].str.replace('Tropical Andes', '')\n",
    "out['Loc'] = out['Loc'].str.replace('Taillon-Gabietous basin', 'Gabietous')\n",
    "out['Loc'] = out['Loc'].str.replace('Lachenpas Sikkim', 'Sikkim')\n",
    "out['Loc'] = out['Loc'].str.replace('Cascade Range', 'Stabler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use developer API key to connect with google maps geocode API\n",
    "GOOGLE_MAP_KEY = 'AIzaSyCxKqtFcC03BU05or-wUQTL15L1NBnsCnU'\n",
    "# GOOGLE_MAP_KEY = 'AIzaSyAIwbssrxa6v2jh5eAivgjewm_O_tPDid0'\n",
    "gmaps = googlemaps.Client(GOOGLE_MAP_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract lat/lon for all data possible\n",
    "geocode_results = []\n",
    "longs = []\n",
    "lats = []\n",
    "\n",
    "for i in xrange(len(out)):\n",
    "    \n",
    "    location = out['Loc'].iloc[i]\n",
    "    country = out['Country'].iloc[i]\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        geocode_result = gmaps.geocode('{}, {}'.format(location, country))\n",
    "#         if geocode_result[0]['geometry']['location']['lat']:\n",
    "        geocode_results.append(geocode_result[0])\n",
    "        lats.append(geocode_result[0]['geometry']['location']['lat'])\n",
    "        longs.append(geocode_result[0]['geometry']['location']['lng'])\n",
    "#         else:\n",
    "#             geocode_result = gmaps.geocode('{}, {}'.format(capitol, country))\n",
    "#             geocode_results.append(geocode_result[0])\n",
    "#             lats.append(geocode_result[0]['geometry']['location']['lat'])\n",
    "#             longs.append(geocode_result[0]['geometry']['location']['lng'])\n",
    "            \n",
    "    except:\n",
    "        geocode_results.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "        longs.append(np.nan)\n",
    "            \n",
    "out['lats'] = lats\n",
    "out['longs'] = longs\n",
    "out['geocode_results'] = geocode_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915 locations total\n",
      "1 locations were not geocoded via automation;\n",
      "   need to manually extract\n"
     ]
    }
   ],
   "source": [
    "tmp = out.loc[out['lats'].isnull()]\n",
    "n_uni = len(tmp['Loc'].unique())\n",
    "print len(out), 'locations total'\n",
    "print n_uni, 'locations were not geocoded via automation;\\n   need to manually extract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Reverse geocode to check if lat/long yield desired addresses\n",
    "\n",
    "# def rev_geo(coords):\n",
    "#     \"\"\"\n",
    "#     Reverse geocode lookup\n",
    "#     \"\"\"\n",
    "#     x = coords[0]\n",
    "#     y = coords[1]\n",
    "#     try:\n",
    "#         a = gmaps.reverse_geocode((x, y))[0]['formatted_address']\n",
    "#     except:\n",
    "#         a = [np.nan, np.nan]\n",
    "    \n",
    "#     return a\n",
    "\n",
    "# out['coords'] = zip(out['lats'], out['longs'])\n",
    "# check = out['coords'].apply(rev_geo).str.split(',', expand=False)\n",
    "# out['check'] = check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if country from reverse geocode matches \n",
    "# with lat/long; note there are contested territories\n",
    "# in China, India, and Pakistan that do not have a country associated\n",
    "# with the reverse geocode.\n",
    "\n",
    "# out['check_country'] = out['check'].str[-1]\n",
    "# out['check_country'] = np.where(out['check_country'] == ' USA', \n",
    "#                                   'United States', \n",
    "#                                   out['check_country'])\n",
    "\n",
    "# out['check_c'] = np.where((out['check_country'].str.strip() == out['Country'].str.strip()) & \n",
    "#                             (out['lats'].notnull()) | \n",
    "#                             (out['Country'] == 'China') | \n",
    "#                             (out['Country'] == 'India') |\n",
    "#                             (out['Country'] == 'Pakistan'), 1, 0)\n",
    "\n",
    "# print out['check_c'].value_counts()\n",
    "\n",
    "# out = out[(out['check_c'] != 0) & out['ISO'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge geocodes with original dataframe\n",
    "\n",
    "out = out.loc[:, ['DOI', 'row_id', 'Country', 'Loc', 'Loc_orig',\n",
    "                  'lats', 'longs', \n",
    "                  'check_country', 'check_x', 'geocodes']]\n",
    "\n",
    "shiny = pd.merge(out, df, on=['DOI', 'row_id'], how='outer')\n",
    "\n",
    "reord_cols = ['DOI', 'Q 1.', 'Q 2.', 'Q 3.']\n",
    "reorder = shiny.loc[:, reord_cols]\n",
    "shiny = shiny.drop(reord_cols, axis=1)\n",
    "shiny = pd.concat([reorder, shiny], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will prep data for shiny app\n",
    "# by assigning column names and dropping duplicate location entries.\n",
    "# In the future, we will have the row number of this dataframe\n",
    "# equal the unique locations AND adaptation features to be\n",
    "# sortable\n",
    "\n",
    "cDict = {'DOI':   'StudyID',  \n",
    "         'Q 1.':  'StudyTitle', \n",
    "         'Q 2.':  'Year',\n",
    "         'Q 3.':  'StudyAuthors',\n",
    "         'Q 21.': 'Scale',\n",
    "         'Q 28.': 'StudyDesc'}\n",
    "\n",
    "cList = list(shiny.columns)\n",
    "shiny.columns = [ cDict.get(item,item) for item in cList ]\n",
    "\n",
    "shiny['StudyLoc'] = shiny['Loc_orig'].str.title() + ', ' + shiny['Country'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enable Type_A to be subsettable by adaptation type\n",
    "\n",
    "dictionary = {\n",
    "    '1': 'Behavioral',\n",
    "    '2': 'Technological',\n",
    "    '3': 'Financial',\n",
    "    '4': 'Institutional',\n",
    "    '5': 'Regulatory',\n",
    "    '6': 'Informational',\n",
    "    '7': 'Infrastructure',\n",
    "    '8': 'Monitoring'\n",
    "}\n",
    "\n",
    "\n",
    "def return_map(dictionary, x):\n",
    "    if type(x) is list:\n",
    "        l = [dictionary.get(item, item)  for item in x]\n",
    "        return \", \".join(str(x).strip() for x in l)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "shiny['Q 23.'] = shiny['Q 23.'].str.replace('.', ',')\n",
    "shiny['Type_A'] = shiny['Q 23.'].str.split(',', expand=False)\n",
    "shiny['Type_A'] = shiny['Type_A'].apply(lambda x: return_map(dictionary, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enable Type_C to be subsettable by adaptation type\n",
    "\n",
    "dictionary = {\n",
    "    '1.0': 'Groundwork',\n",
    "    '2.0': 'Partially implemented',\n",
    "    '3.0': 'Fully implemented and ongoing',\n",
    "    '4.0': 'Fully implemented and finished',\n",
    "    '5.0': 'Evaluated',\n",
    "    '6.0': 'Indeterminate'\n",
    "}\n",
    "\n",
    "shiny['Type_C'] = shiny['Q 25.'].astype('str').map(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enables dataframe to be subsetable by adaptation type\n",
    "\n",
    "subs = ['StudyID', 'StudyTitle', 'StudyAuthors', 'StudyLoc', \n",
    "        'lats', 'longs', 'Scale', 'Year', \n",
    "        'Type_A', 'Type_B', 'Type_C', 'StudyDesc']\n",
    "\n",
    "df_shiny = shiny.loc[:, subs]\n",
    "\n",
    "df_shiny['StudyDesc'] = np.where(df_shiny['StudyDesc'].isnull(), \n",
    "                                 'No description available', \n",
    "                                 df_shiny['StudyDesc'])\n",
    "\n",
    "# Used to test geocoding accuracy\n",
    "df_shiny['Type_B'] = np.where(df_shiny['Scale'] > 4, 'Regional', 'Local')\n",
    "\n",
    "# df_shiny = df_shiny.drop_duplicates(subset=subs[:-3])  # This subset helps omit duplicate studysite entries. Redact once ready to deploy\n",
    "# df_shiny = df_shiny.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract Abstract from endnote database\n",
    "\n",
    "xml = '../data/Article_database/EndNote_metadata.xml'\n",
    "\n",
    "with open(xml) as fd:\n",
    "    doc = xmltodict.parse(fd.read())\n",
    "\n",
    "dois = []\n",
    "abstracts = []\n",
    "\n",
    "for i in xrange(len(doc['xml']['records']['record'])):\n",
    "    \n",
    "    try:\n",
    "        doi = doc['xml']['records']['record'][i]['electronic-resource-num']['style']['#text']\n",
    "        abstract = doc['xml']['records']['record'][i]['abstract']['style']['#text']\n",
    "    except:\n",
    "        doi = np.nan\n",
    "        abstract = 'No abstract avialable'\n",
    "    \n",
    "    dois.append(doi)\n",
    "    abstracts.append(abstract)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    'StudyID': dois,\n",
    "    'Abstract': abstracts\n",
    "})\n",
    "\n",
    "\n",
    "out['StudyID'] = out['StudyID'].str.replace('dx.doi.org/', '')\n",
    "out['StudyID'] = out['StudyID'].str.replace('doi.org/', '')\n",
    "\n",
    "df_shiny['StudyID'] = df_shiny['StudyID'].str.replace('dx.doi.org/', '')\n",
    "df_shiny['StudyID'] = df_shiny['StudyID'].str.replace('doi.org/', '')\n",
    "df_shiny['StudyID'] = np.where(df_shiny['StudyID'].str.slice(0,3) == \"10.\", df_shiny['StudyID'], np.nan)\n",
    "\n",
    "df_shiny = pd.merge(df_shiny, out, on='StudyID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in any manually extracted abstracts/summaries\n",
    "df_manual_abstracts = pd.read_csv('../data/Manual_Abtracts.csv', encoding=\"utf-8\")\n",
    "df_shiny = pd.merge(df_shiny, df_manual_abstracts, on=['StudyTitle'], how='left')\n",
    "df_shiny['Abstract'] = np.where(\n",
    "    (df_shiny['Abstract'].str.contains('No abstract')) & (df_shiny['Abstract_manual'].notnull()), \n",
    "    df_shiny['Abstract_manual'], \n",
    "    df_shiny['Abstract'])\n",
    "\n",
    "del df_shiny['Abstract_manual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searchable variable that concats citation, \n",
    "# abstract, adaptation type, adaptation response, methods, \n",
    "# and other relevant keywords\n",
    "\n",
    "df_shiny['freeTextLookup'] = (df_shiny['Abstract'] + ', ' +\n",
    "                              df_shiny['StudyTitle'] + ', ' +\n",
    "                              df_shiny['StudyAuthors'] + ', ' +\n",
    "                              df_shiny['StudyLoc'])\n",
    "\n",
    "df_shiny['freeTextLookup'] = df_shiny['freeTextLookup'].str.replace('[^0-9a-zA-Z]+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Last minute cleaning\n",
    "df_shiny['Type_A'] = df_shiny['Type_A'].str.replace(', 7, 8', 'Other')\n",
    "df_shiny['Type_A'] = df_shiny['Type_A'].str.replace(' ', '')\n",
    "df_shiny['Type_A'] = df_shiny['Type_A'].str.replace('TechnologicalOther', 'Technological, Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manually (re)geocode any points needed here\n",
    "df_shiny['lats'] = np.where(df_shiny['StudyLoc'] == 'Altiplano, Peru', -18.0, df_shiny['lats'])\n",
    "df_shiny['longs'] = np.where(df_shiny['StudyLoc'] == 'Altiplano, Peru', -68.0, df_shiny['longs'])\n",
    "df_shiny['StudyLoc'] = np.where(df_shiny['StudyLoc'] == -68.0, 'Altiplano Plateau, Bolivia', df_shiny['StudyLoc'])\n",
    "df_shiny['StudyLoc'] = np.where(df_shiny['StudyLoc'] == 'Altiplano, Peru', 'Altiplano Plateau, Bolivia', df_shiny['StudyLoc'])\n",
    "\n",
    "df_shiny.loc[df_shiny['StudyTitle'] == 'Kazakhstan - Overview of climate change activities']\n",
    "df_shiny['lats'] = np.where(df_shiny['StudyTitle'] == 'Kazakhstan - Overview of climate change activities', \n",
    "                            42.428763, df_shiny['lats'])\n",
    "df_shiny['longs'] = np.where(df_shiny['StudyTitle'] == 'Kazakhstan - Overview of climate change activities', \n",
    "                             70.727839, df_shiny['longs'])\n",
    "\n",
    "df_shiny['lats'] = np.where(df_shiny['StudyLoc'] == 'Rio Negro, Argentina', -40.686059, df_shiny['lats'])\n",
    "df_shiny['longs'] = np.where(df_shiny['StudyLoc'] == 'Rio Negro, Argentina', -70.992455, df_shiny['longs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_shiny.to_csv('../shinyapp/data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2_geocode_update",
   "language": "python",
   "name": "py2_geocode_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
